{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torchsummary import summary\n",
    "STEP =256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7676, 18176, 1) (7676, 71)\n"
     ]
    }
   ],
   "source": [
    "#Padded input data , labels are arrays which are also padded to maximum size , \n",
    "#its not one hot encoded. \n",
    "data = np.load('train.npz')\n",
    "train_x = data['a']\n",
    "train_y = data['b']\n",
    "print(train_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just an generator for training.\n",
    "def data_generator(batch_size, x, y):\n",
    "    num_examples = len(x)\n",
    "    examples = zip(x, y)\n",
    "    examples = sorted(examples, key = lambda x: x[0].shape[0])\n",
    "    end = num_examples - batch_size + 1\n",
    "    batches = [examples[i:i+batch_size]\n",
    "                for i in range(0, end, batch_size)]\n",
    "    random.shuffle(batches)\n",
    "    batchlen = int(len(batches))\n",
    "  \n",
    "    i =0\n",
    "    while True:\n",
    "\n",
    "        if i >= batchlen:\n",
    "               return\n",
    "        #print(i)\n",
    "        x, y = zip(*batches[i])\n",
    "        yield np.asarray(x), np.asarray(y)\n",
    "        i = i +1\n",
    "        \n",
    "            #yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bn_Relu(nn.Module):\n",
    "    def __init__(self, output,dropout=0):\n",
    "        super(Bn_Relu,self).__init__()\n",
    "        self.bn =  nn.BatchNorm1d(output)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = None\n",
    "        if dropout > 0:\n",
    "           self.drop = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out = self.bn(x)\n",
    "        out = self.relu(out)\n",
    "        if self.drop is not None:\n",
    "            out = self.drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self,prev_filters,num_filters,subsample_length,block_index,dropout):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.subsample_length = subsample_length\n",
    "        self.max1 = nn.MaxPool1d(kernel_size=subsample_length)\n",
    "        self.zero_pad = (block_index % 4) == 0 and block_index > 0\n",
    "        #if zero_pad is True:\n",
    "            #shortcut = Lambda(zeropad, output_shape=zeropad_output_shape)(shortcut)\n",
    "        print(\"Conv 1\" ,prev_filters,num_filters,subsample_length)\n",
    "        self.conv1 = nn.Conv1d(prev_filters, num_filters, 3, subsample_length,padding=1)\n",
    "        self.bn1 = None\n",
    "        if(block_index !=0):\n",
    "              self.bn1 = Bn_Relu(num_filters, dropout )\n",
    "        print(\"Conv 2\" ,num_filters)\n",
    "        self.downsample = None\n",
    "        if( subsample_length != 1 ) or (prev_filters != num_filters):\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(prev_filters, num_filters, 3, subsample_length,padding=1),\n",
    "                nn.BatchNorm1d(num_filters))\n",
    "            \n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters,3, 1,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        iden = x #self.max1(x)\n",
    "                       \n",
    "        #print (\"Shape After max \",iden.shape,self.subsample_length)    \n",
    "        out = self.conv1(x)\n",
    "        if self.bn1 is not None:\n",
    "            out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            iden = self.downsample(iden)\n",
    "        #print (\"out Before \",out.shape,iden.shape)\n",
    "        \n",
    "        \n",
    "        #print (\"Shape first ,second\",iden.shape, out.shape)\n",
    "        out += iden\n",
    "        return out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_filters_at_index(index, num_start_filters):\n",
    "    return 2**int(index / 4) \\\n",
    "        * num_start_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,dropout):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layers1 = nn.Conv1d(1, 32, 16, 1)\n",
    "        self.bn1 = Bn_Relu(32)\n",
    "        layers = []\n",
    "        res = [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
    "        #res = [1,2,1]\n",
    "        prev_filter =32\n",
    "        self.debug =0;\n",
    "        for index, subsample_length in enumerate(res):\n",
    "            num_filters = get_num_filters_at_index(index, 32 )\n",
    "            print(\"filter length\",prev_filter,num_filters)\n",
    "            x =  ResnetBlock(prev_filter,num_filters,subsample_length,index,dropout)\n",
    "            y = Bn_Relu(num_filters,dropout)\n",
    "            prev_filter = num_filters\n",
    "            layers.append(x)\n",
    "            layers.append(y)\n",
    "        self.resLayer = nn.Sequential(*layers)\n",
    "        self.deconv = nn.ConvTranspose1d(256,256,2)\n",
    "        self.linear1 = nn.Linear(256,4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(self.debug)\n",
    "        self.debug +=1\n",
    "        x= self.layers1(x)\n",
    "        x= self.bn1(x)\n",
    "        x = self.resLayer(x)\n",
    "        #print(\"shape before\",x.shape)\n",
    "        #x = F.pad(x, (0,1), \"constant\", 0) \n",
    "        #print(\"shape before\",x.shape)\n",
    "        #x = self.deconv(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        #y = x.view(x.size()[0], -1);\n",
    "        #print (\"final shape \", x.shape)\n",
    "        #return F.log_softmax(self.linear1(x.view(x.size()[0], -1)), dim=1)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        #return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\fresh\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_fn = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter length 32 32\n",
      "Conv 1 32 32 1\n",
      "Conv 2 32\n",
      "filter length 32 32\n",
      "Conv 1 32 32 2\n",
      "Conv 2 32\n",
      "filter length 32 32\n",
      "Conv 1 32 32 1\n",
      "Conv 2 32\n",
      "filter length 32 32\n",
      "Conv 1 32 32 2\n",
      "Conv 2 32\n",
      "filter length 32 64\n",
      "Conv 1 32 64 1\n",
      "Conv 2 64\n",
      "filter length 64 64\n",
      "Conv 1 64 64 2\n",
      "Conv 2 64\n",
      "filter length 64 64\n",
      "Conv 1 64 64 1\n",
      "Conv 2 64\n",
      "filter length 64 64\n",
      "Conv 1 64 64 2\n",
      "Conv 2 64\n",
      "filter length 64 128\n",
      "Conv 1 64 128 1\n",
      "Conv 2 128\n",
      "filter length 128 128\n",
      "Conv 1 128 128 2\n",
      "Conv 2 128\n",
      "filter length 128 128\n",
      "Conv 1 128 128 1\n",
      "Conv 2 128\n",
      "filter length 128 128\n",
      "Conv 1 128 128 2\n",
      "Conv 2 128\n",
      "filter length 128 256\n",
      "Conv 1 128 256 1\n",
      "Conv 2 256\n",
      "filter length 256 256\n",
      "Conv 1 256 256 2\n",
      "Conv 2 256\n",
      "filter length 256 256\n",
      "Conv 1 256 256 1\n",
      "Conv 2 256\n",
      "filter length 256 256\n",
      "Conv 1 256 256 2\n",
      "Conv 2 256\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(dropout =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (layers1): Conv1d(1, 32, kernel_size=(16,), stride=(1,))\n",
       "  (bn1): Bn_Relu(\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (resLayer): Sequential(\n",
       "    (0): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (1): Bn_Relu(\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (2): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (3): Bn_Relu(\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (4): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (5): Bn_Relu(\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (6): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (7): Bn_Relu(\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (8): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (9): Bn_Relu(\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (10): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (11): Bn_Relu(\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (13): Bn_Relu(\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (15): Bn_Relu(\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (16): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (17): Bn_Relu(\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (18): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (19): Bn_Relu(\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (20): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (21): Bn_Relu(\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (22): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (23): Bn_Relu(\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (24): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (25): Bn_Relu(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (26): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (27): Bn_Relu(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (28): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (29): Bn_Relu(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "    (30): ResnetBlock(\n",
       "      (max1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (bn1): Bn_Relu(\n",
       "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "        (drop): Dropout(p=0.2)\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (31): Bn_Relu(\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (drop): Dropout(p=0.2)\n",
       "    )\n",
       "  )\n",
       "  (deconv): ConvTranspose1d(256, 256, kernel_size=(2,), stride=(1,))\n",
       "  (linear1): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(epoch,batchsize):\n",
    "    model.train()    \n",
    "    for epoch in range(epoch):\n",
    "        print('*'*10)\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        tloss = 0.0\n",
    "        taccu = 0.0\n",
    "        train_gen = data_generator(batchsize,train_x,train_y)\n",
    "        for x,y in train_gen:\n",
    "            #print (x.shape,y.shape)\n",
    "            inp = np.transpose(x, (0, 2, 1))\n",
    "            inp =torch.from_numpy(inp).requires_grad_()\n",
    "            inpTensor= inp.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inpTensor)\n",
    "            labels = torch.from_numpy(y).long()\n",
    "            labels = labels.cuda()\n",
    "            loss = loss_fn(outputs.permute(0,2,1),labels)\n",
    "            #loss = loss_fn(outputs.view(outputs.shape[0]*outputs.shape[1],-1), labels.view(labels.shape[0]*labels.shape[1]))\n",
    "            tloss += loss.item()\n",
    "            pred = torch.max(outputs, -1)[1]\n",
    "            taccu += (pred == labels).sum().item()\n",
    "            #print(\"loss \", loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #break\n",
    "            \n",
    "        print('Loss: {}'.format(tloss))\n",
    "        print('Accu: {}'.format(taccu/len(train_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidateModel(batchsize):\n",
    "    model.eval()\n",
    "    tloss = 0.0\n",
    "    taccu = 0.0\n",
    "    valid_gen = data_generator(batchsize,valid_x,valid_y)\n",
    "    for x,y in valid_gen:\n",
    "        inp = np.transpose(x, (0, 2, 1))\n",
    "        inp =torch.from_numpy(inp)\n",
    "        inpTensor= inp.cuda()\n",
    "        outputs = model(inpTensor)\n",
    "        labels = torch.from_numpy(y).long()\n",
    "        labels = labels.cuda()\n",
    "        loss = loss_fn(outputs.permute(0,2,1),labels)\n",
    "        #loss = loss_fn(outputs.view(outputs.shape[0]*outputs.shape[1],-1), labels.view(labels.shape[0]*labels.shape[1]))\n",
    "        tloss += loss.item()\n",
    "        pred = torch.max(outputs, -1)[1]\n",
    "        taccu += (pred == labels).sum().item()\n",
    "    print('Loss: {}'.format(tloss))\n",
    "    print('Accu: {}'.format(taccu/len(valid_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(852, 18176, 1) (852, 71)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('valid.npz')\n",
    "valid_x = data['a']\n",
    "valid_y = data['b']\n",
    "print(valid_x.shape,valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(os.path.exists(\"model-latest.pth\")) :\n",
    "\tmodel.load_state_dict(torch.load(\"model-latest.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 69546.4031829834\n",
      "Accu: 66.92339760291819\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "TrainModel(1,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 262899.7056274414\n",
      "Accu: 55.71430432516936\n"
     ]
    }
   ],
   "source": [
    "TrainModel(1,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 22425.94314956665\n",
      "Accu: 63.44600938967136\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ValidateModel(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"test-1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 233093.6732788086\n",
      "Accu: 58.023579989577904\n",
      "**********\n",
      "Epoch: 1\n",
      "Loss: 208672.001953125\n",
      "Accu: 59.460005211047424\n",
      "**********\n",
      "Epoch: 2\n",
      "Loss: 199116.2645263672\n",
      "Accu: 59.92509119332986\n",
      "**********\n",
      "Epoch: 3\n",
      "Loss: 199087.25582885742\n",
      "Accu: 59.963262115685254\n",
      "**********\n",
      "Epoch: 4\n",
      "Loss: 193717.21255493164\n",
      "Accu: 60.28947368421053\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "TrainModel(5,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 22302.803985595703\n",
      "Accu: 60.063380281690144\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ValidateModel(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"test-1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 153722.51000976562\n",
      "Accu: 63.23150078165711\n",
      "**********\n",
      "Epoch: 1\n",
      "Loss: 151669.26217651367\n",
      "Accu: 63.30875455966649\n",
      "**********\n",
      "Epoch: 2\n",
      "Loss: 150966.83184814453\n",
      "Accu: 63.22146951537259\n",
      "**********\n",
      "Epoch: 3\n",
      "Loss: 148333.6430053711\n",
      "Accu: 63.451146430432516\n",
      "**********\n",
      "Epoch: 4\n",
      "Loss: 146318.73986816406\n",
      "Accu: 63.577384054194894\n",
      "**********\n",
      "Epoch: 5\n",
      "Loss: 143104.78533935547\n",
      "Accu: 63.5872850442939\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "TrainModel(6,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 21864.313079833984\n",
      "Accu: 60.225352112676056\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ValidateModel(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"test-1.pth\")\n",
    "model.load_state_dict(torch.load(\"test-1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 142182.44482421875\n",
      "Accu: 63.65046899426785\n",
      "**********\n",
      "Epoch: 1\n",
      "Loss: 140148.30081176758\n",
      "Accu: 63.661021365294424\n",
      "**********\n",
      "Epoch: 2\n",
      "Loss: 141206.31518554688\n",
      "Accu: 63.642391870766026\n",
      "**********\n",
      "Epoch: 3\n",
      "Loss: 141265.24829101562\n",
      "Accu: 63.68981240229286\n",
      "**********\n",
      "Epoch: 4\n",
      "Loss: 137422.86274719238\n",
      "Accu: 63.84275664408546\n",
      "**********\n",
      "Epoch: 5\n",
      "Loss: 137796.7368774414\n",
      "Accu: 63.823736321000524\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "TrainModel(6,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18555.244667053223\n",
      "Accu: 62.570422535211264\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ValidateModel(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"test-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"test-2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 0\n",
      "Loss: 136388.07900238037\n",
      "Accu: 63.86099531005732\n",
      "**********\n",
      "Epoch: 1\n",
      "Loss: 132161.95597076416\n",
      "Accu: 64.10265763418447\n",
      "**********\n",
      "Epoch: 2\n",
      "Loss: 131530.73462677002\n",
      "Accu: 64.1323606044815\n",
      "**********\n",
      "Epoch: 3\n",
      "Loss: 129756.62490463257\n",
      "Accu: 64.34236581552892\n",
      "**********\n",
      "Epoch: 4\n",
      "Loss: 126288.5089263916\n",
      "Accu: 64.35330901511203\n",
      "**********\n",
      "Epoch: 5\n",
      "Loss: 126232.15660095215\n",
      "Accu: 64.3882230328296\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "TrainModel(6,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15539.197761535645\n",
      "Accu: 63.617370892018776\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ValidateModel(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"test-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
